[2025-04-20 21:31:02,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:02,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:02,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:02,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:02,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:02,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:02,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:02,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:03,863] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:03,863] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:03,863] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:03,863] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:03,863] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:03,863] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:03,863] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 21:31:03,863] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m FP Quantizer is using an untested triton version (3.2.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m gds is not compatible with ROCM
gds .................... [93m[NO][0m ....... [93m[NO][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn is not compatible with ROCM
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch']
torch version .................... 2.2.0.dev20231106+rocm5.7
deepspeed install path ........... ['/lustre/orion/gen150/scratch/pinaster/moe-arch/DeepSpeed/deepspeed']
deepspeed info ................... 0.15.5+unknown, unknown, unknown
torch cuda version ............... None
torch hip version ................ 5.7.31921-d1770ee1b
nvcc version ..................... None
deepspeed wheel compiled w. ...... torch 2.2, hip 5.7
shared memory (/dev/shm) size .... 251.20 GB
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
using mpi
INFO: overriding default arguments for tokenizer_type:None                    with tokenizer_type:GPT2BPETokenizer
using world size: 8, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_attention ............................ False
  checkpoint_gating ............................... False
  checkpoint_in_cpu ............................... False
  checkpoint_intermediate ......................... False
  checkpoint_layernorm ............................ False
  checkpoint_num_layers ........................... 1
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning_legacy ...................... False
  data_cache_path ................................. None
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 8
  data_path ....................................... ['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_checkpointed_activations ............. False
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  ds_sequence_parallel_size ....................... 1
  embedding_path .................................. None
  embedding_weights_in_fp32 ....................... False
  empty_unused_memory_level ....................... 0
  enable_expert_sequence_parallelism .............. False
  enable_expert_tensor_parallelism ................ False
  encoder_num_layers .............................. 28
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 0
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_interval ................................. 1
  ffn_hidden_size ................................. 1408
  finetune ........................................ False
  force_ds_sequence_parallel ...................... False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 256
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00012
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 375000000
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  master_addr ..................................... 100.64.201.123
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  mem_efficient_ln ................................ True
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 8
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.25
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  normalization ................................... layernorm
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [64]
  num_experts_switch .............................. None
  num_experts_teacher ............................. [1]
  num_key_value_heads ............................. 16
  num_layers ...................................... 28
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_shared_experts .............................. [0]
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  profile_backward ................................ False
  profile_name .................................... None
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  rbd_mesh_size ................................... 8
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  return_data_index ............................... False
  rotary_percent .................................. 1.0
  sample_rate ..................................... 1.0
  save ............................................ checkpoints/smore-test
  save_interval ................................... 50
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  split ........................................... 98,2,0
  split_transformers .............................. False
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  test_data_path .................................. None
  tile_factor ..................................... 1
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 6
  train_data_exact_num_epochs ..................... None
  train_data_path ................................. None
  train_desc_path ................................. None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 100
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... 300000000000
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  universal_checkpoint ............................ False
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_dataset_only ................................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_flash_attn_triton ........................... False
  use_flash_attn_v1 ............................... True
  use_flash_attn_v2 ............................... True
  use_one_sent_docs ............................... False
  use_pft ......................................... True
  use_pin_memory .................................. False
  use_rbd ......................................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. True
  use_tutel ....................................... False
  use_tutel_moe ................................... False
  use_uneven_all_to_all ........................... True
  using_mpi ....................................... True
  using_torchrun .................................. False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json
  vocab_size ...................................... None
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 8
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m FP Quantizer is using an untested triton version (3.2.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m gds is not compatible with ROCM
gds .................... [93m[NO][0m ....... [93m[NO][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn is not compatible with ROCM
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch']
torch version .................... 2.2.0.dev20231106+rocm5.7
deepspeed install path ........... ['/lustre/orion/gen150/scratch/pinaster/moe-arch/DeepSpeed/deepspeed']
deepspeed info ................... 0.15.5+unknown, unknown, unknown
torch cuda version ............... None
torch hip version ................ 5.7.31921-d1770ee1b
nvcc version ..................... None
deepspeed wheel compiled w. ...... torch 2.2, hip 5.7
shared memory (/dev/shm) size .... 251.20 GB
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
using mpi
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
world_size, rank, master_addr, local_rank: 8 0 10.128.148.0 0
> initializing torch distributed ...
args values: Namespace(num_layers=28, encoder_num_layers=28, decoder_num_layers=None, num_experts=[64], num_shared_experts=[0], mlp_type='standard', topk=6, expert_interval=1, hidden_size=2048, ffn_hidden_size=1408, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, use_uneven_all_to_all=True, use_rbd=False, rbd_mesh_size=8, master_addr='100.64.201.123', attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=4, global_batch_size=256, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=100, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=1, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.25, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=True, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, no_pipeline_parallel=True, use_tutel=False, use_pft=True, use_tutel_moe=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='checkpoints/smore-test', save_interval=50, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, enable_expert_sequence_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m FP Quantizer is using an untested triton version (3.2.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m gds is not compatible with ROCM
gds .................... [93m[NO][0m ....... [93m[NO][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn is not compatible with ROCM
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch']
torch version .................... 2.2.0.dev20231106+rocm5.7
deepspeed install path ........... ['/lustre/orion/gen150/scratch/pinaster/moe-arch/DeepSpeed/deepspeed']
deepspeed info ................... 0.15.5+unknown, unknown, unknown
torch cuda version ............... None
torch hip version ................ 5.7.31921-d1770ee1b
nvcc version ..................... None
deepspeed wheel compiled w. ...... torch 2.2, hip 5.7
shared memory (/dev/shm) size .... 251.20 GB
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
using mpi
world_size, rank, master_addr, local_rank: 8 2 10.128.148.0 2
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m FP Quantizer is using an untested triton version (3.2.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m gds is not compatible with ROCM
gds .................... [93m[NO][0m ....... [93m[NO][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn is not compatible with ROCM
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch']
torch version .................... 2.2.0.dev20231106+rocm5.7
deepspeed install path ........... ['/lustre/orion/gen150/scratch/pinaster/moe-arch/DeepSpeed/deepspeed']
deepspeed info ................... 0.15.5+unknown, unknown, unknown
torch cuda version ............... None
torch hip version ................ 5.7.31921-d1770ee1b
nvcc version ..................... None
deepspeed wheel compiled w. ...... torch 2.2, hip 5.7
shared memory (/dev/shm) size .... 251.20 GB
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
using mpi
world_size, rank, master_addr, local_rank: 8 5 10.128.148.0 5
world_size, rank, master_addr, local_rank: 8 7 10.128.148.0 7
args values: Namespace(num_layers=28, encoder_num_layers=28, decoder_num_layers=None, num_experts=[64], num_shared_experts=[0], mlp_type='standard', topk=6, expert_interval=1, hidden_size=2048, ffn_hidden_size=1408, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, use_uneven_all_to_all=True, use_rbd=False, rbd_mesh_size=8, master_addr='100.64.201.123', attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=4, global_batch_size=256, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=100, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=1, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.25, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=True, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, no_pipeline_parallel=True, use_tutel=False, use_pft=True, use_tutel_moe=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='checkpoints/smore-test', save_interval=50, no_save_optim=None, no_save_rngcpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, using_torchrun=False, profile_name=None, eval_iters=0, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json', merge_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, checkpoint_intermediate=False, checkpoint_layernorm=False, checkpoint_attention=False, checkpoint_gating=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m FP Quantizer is using an untested triton version (3.2.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m gds is not compatible with ROCM
gds .................... [93m[NO][0m ....... [93m[NO][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn is not compatible with ROCM
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch']
torch version .................... 2.2.0.dev20231106+rocm5.7
deepspeed install path ........... ['/lustre/orion/gen150/scratch/pinaster/moe-arch/DeepSpeed/deepspeed']
deepspeed info ................... 0.15.5+unknown, unknown, unknown
torch cuda version ............... None
torch hip version ................ 5.7.31921-d1770ee1b
nvcc version ..................... None
deepspeed wheel compiled w. ...... torch 2.2, hip 5.7
shared memory (/dev/shm) size .... 251.20 GB
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
using mpi
world_size, rank, master_addr, local_rank: 8 1 10.128.148.0 1
args values: Namespace(num_layers=28, encoder_num_layers=28, decoder_num_layers=None, num_experts=[64], num_shared_experts=[0], mlp_type='standard', topk=6, expert_interval=1, hidden_size=2048, ffn_hidden_size=1408, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, use_uneven_all_to_all=True, use_rbd=False, rbd_mesh_size=8, master_addr='100.64.201.123', attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=4, global_batch_size=256, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=100, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=1, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.25, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=True, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, no_pipeline_parallel=True, use_tutel=False, use_pft=True, use_tutel_moe=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='checkpoints/smore-test', save_interval=50, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, enable_expert_sequence_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m FP Quantizer is using an untested triton version (3.2.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m gds is not compatible with ROCM
gds .................... [93m[NO][0m ....... [93m[NO][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn is not compatible with ROCM
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch']
torch version .................... 2.2.0.dev20231106+rocm5.7
deepspeed install path ........... ['/lustre/orion/gen150/scratch/pinaster/moe-arch/DeepSpeed/deepspeed']
deepspeed info ................... 0.15.5+unknown, unknown, unknown
torch cuda version ............... None
torch hip version ................ 5.7.31921-d1770ee1b
nvcc version ..................... None
deepspeed wheel compiled w. ...... torch 2.2, hip 5.7
shared memory (/dev/shm) size .... 251.20 GB
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
using mpi
world_size, rank, master_addr, local_rank: 8 3 10.128.148.0 3
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m FP Quantizer is using an untested triton version (3.2.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m gds is not compatible with ROCM
gds .................... [93m[NO][0m ....... [93m[NO][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn is not compatible with ROCM
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch']
torch version .................... 2.2.0.dev20231106+rocm5.7
deepspeed install path ........... ['/lustre/orion/gen150/scratch/pinaster/moe-arch/DeepSpeed/deepspeed']
deepspeed info ................... 0.15.5+unknown, unknown, unknown
torch cuda version ............... None
torch hip version ................ 5.7.31921-d1770ee1b
nvcc version ..................... None
deepspeed wheel compiled w. ...... torch 2.2, hip 5.7
shared memory (/dev/shm) size .... 251.20 GB
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
using mpi
world_size, rank, master_addr, local_rank: 8 4 10.128.148.0 4
args values: Namespace(num_layers=28, encoder_num_layers=28, decoder_num_layers=None, num_experts=[64], num_shared_experts=[0], mlp_type='standard', topk=6, expert_interval=1, hidden_size=2048, ffn_hidden_size=1408, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, use_uneven_all_to_all=True, use_rbd=False, rbd_mesh_size=8, master_addr='100.64.201.123', attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=4, global_batch_size=256, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=100, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=1, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.25, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=True, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, no_pipeline_parallel=True, use_tutel=False, use_pft=True, use_tutel_moe=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='checkpoints/smore-test', save_interval=50, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, enable_expert_sequence_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m FP Quantizer is using an untested triton version (3.2.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m gds is not compatible with ROCM
gds .................... [93m[NO][0m ....... [93m[NO][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn is not compatible with ROCM
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch']
torch version .................... 2.2.0.dev20231106+rocm5.7
deepspeed install path ........... ['/lustre/orion/gen150/scratch/pinaster/moe-arch/DeepSpeed/deepspeed']
deepspeed info ................... 0.15.5+unknown, unknown, unknown
torch cuda version ............... None
torch hip version ................ 5.7.31921-d1770ee1b
nvcc version ..................... None
deepspeed wheel compiled w. ...... torch 2.2, hip 5.7
shared memory (/dev/shm) size .... 251.20 GB
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
using mpi
world_size, rank, master_addr, local_rank: 8 6 10.128.148.0 6
=None, load=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, enable_expert_sequence_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, using_torchrun=False, profile_name=None, eval_iters=0, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json', merge_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activationconfig='ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json', deepscale=False, deepscale_config=None, using_mpi=True, rank=0, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, ds_pipeline_enabled=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=50304)
[2025-04-20 21:31:07,094] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-04-20 21:31:07,094] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
args values: Namespace(num_layers=28, encoder_num_layers=28, decoder_num_layers=None, num_experts=[64], num_shared_experts=[0], mlp_type='standard', topk=6, expert_interval=1, hidden_size=2048, ffn_hidden_size=1408, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, use_uneven_all_to_all=True, use_rbd=False, rbd_mesh_size=8, master_addr='100.64.201.123', attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=4, global_batch_size=256, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=100, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=1, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.25, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=True, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, no_pipeline_parallel=True, use_tutel=False, use_pft=True, use_tutel_moe=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='checkpoints/smore-test', save_interval=50, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, enable_expert_sequence_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, using_torchrun=False, profile_name=None, eval_iters=0, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json', merge_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, checkpoint_intermediate=False, checkpoint_layernorm=False, checkpoint_attention=False, checkpoint_gating=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_args values: Namespace(num_layers=28, encoder_num_layers=28, decoder_num_layers=None, num_experts=[64], num_shared_experts=[0], mlp_type='standard', topk=6, expert_interval=1, hidden_size=2048, ffn_hidden_size=1408, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, use_uneven_all_to_all=True, use_rbd=False, rbd_mesh_size=8, master_addr='100.64.201.123', attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=4, global_batch_size=256, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=100, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=1, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.25, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=True, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, no_pipeline_parallel=True, use_tutel=False, use_pft=True, use_tutel_moe=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='checkpoints/smore-test', save_interval=50, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, enable_expert_sequence_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_args values: Namespace(num_layers=28, encoder_num_layers=28, decoder_num_layers=None, num_experts=[64], num_shared_experts=[0], mlp_type='standard', topk=6, expert_interval=1, hidden_size=2048, ffn_hidden_size=1408, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, use_uneven_all_to_all=True, use_rbd=False, rbd_mesh_size=8, master_addr='100.64.201.123', attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=4, global_batch_size=256, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=100, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=1, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.25, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=True, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, no_pipeline_parallel=True, use_tutel=False, use_pft=True, use_tutel_moe=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='checkpoints/smore-test', save_interval=50, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, enable_expert_sequence_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, using_torchrun=False, profile_name=None, eval_iters=0, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json', merge_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, checkpoint_intermediate=False, checkpoint_layernorm=False, checkpoint_attention=False, checkpoint_gating=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_args values: Namespace(num_layers=28, encoder_num_layers=28, decoder_num_layers=None, num_experts=[64], num_shared_experts=[0], mlp_type='standard', topk=6, expert_interval=1, hidden_size=2048, ffn_hidden_size=1408, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=True, rotary_percent=1.0, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, use_uneven_all_to_all=True, use_rbd=False, rbd_mesh_size=8, master_addr='100.64.201.123', attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=4, global_batch_size=256, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=100, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=1, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/lustre/orion/gen150/scratch/pinaster/moe-arch/system-benchmark/deepseek-style/eval-scripts/hybridSP/weak_scaling/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true_login08.frontier.olcf.ornl.gov_2025.04.20-21.30.53', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.25, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=True, use_flash_attn_v2=True, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, no_pipeline_parallel=True, use_tutel=False, use_pft=True, use_tutel_moe=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='checkpoints/smore-test', save_interval=50, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, enable_expert_sequence_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_s=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, checkpoint_intermediate=False, checkpoint_layernorm=False, checkpoint_attention=False, checkpoint_gating=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config='ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json', deepscale=False, deepscale_config=None, using_mpi=True, rank=7, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, ds_pipeline_enabled=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=50304)
[2025-04-20 21:31:07,094] [INFO] [comm.py:652:init_distributed] cdb=None
cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, using_torchrun=False, profile_name=None, eval_iters=0, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json', merge_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, checkpoint_intermediate=False, checkpoint_layernorm=False, checkpoint_attention=False, checkpoint_gating=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config='ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json', deepscale=False, deepscale_config=None, using_mpi=True, rank=2, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, ds_pipeline_enabled=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=50304)
[2025-04-20 21:31:07,094] [INFO] [comm.py:652:init_distributed] cdb=None
cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, using_torchrun=False, profile_name=None, eval_iters=0, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json', merge_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, checkpoint_intermediate=False, checkpoint_layernorm=False, checkpoint_attention=False, checkpoint_gating=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, using_torchrun=False, profile_name=None, eval_iters=0, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json', merge_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, checkpoint_intermediate=False, checkpoint_layernorm=False, checkpoint_attention=False, checkpoint_gating=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config='ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json', deepscale=False, deepscale_config=None, using_mpi=True, rank=5, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, ds_pipeline_enabled=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=50304)
[2025-04-20 21:31:07,094] [INFO] [comm.py:652:init_distributed] cdb=None
cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, using_torchrun=False, profile_name=None, eval_iters=0, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/lustre/orion/world-shared/stf218/sajal/mtds/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-vocab.json', merge_file='/lustre/orion/gen150/scratch/pinaster/test2/Megatron-DeepSpeed-ORNL/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, checkpoint_intermediate=False, checkpoint_layernorm=False, checkpoint_attention=False, checkpoint_gating=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config='ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json', deepscale=False, deepscale_config=None, using_mpi=True, rank=1, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, ds_pipeline_enabled=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=50304)
[2025-04-20 21:31:07,094] [INFO] [comm.py:652:init_distributed] cdb=None
config='ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json', deepscale=False, deepscale_config=None, using_mpi=True, rank=3, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, ds_pipeline_enabled=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=50304)
[2025-04-20 21:31:07,094] [INFO] [comm.py:652:init_distributed] cdb=None
config='ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json', deepscale=False, deepscale_config=None, using_mpi=True, rank=4, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, ds_pipeline_enabled=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=50304)
[2025-04-20 21:31:07,094] [INFO] [comm.py:652:init_distributed] cdb=None
config='ds_config_gpt_Zero0_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-256-gpus-8-mp-1-pp-1-test-ep-64-mlc-0.01-cap-1.0-drop-true.json', deepscale=False, deepscale_config=None, using_mpi=True, rank=6, world_size=8, transformer_pipeline_model_parallel_size=1, data_parallel_size=8, ds_pipeline_enabled=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=True, padded_vocab_size=50304)
[2025-04-20 21:31:07,094] [INFO] [comm.py:652:init_distributed] cdb=None
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier09467.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier09467.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier09467.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier09467.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier09467.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier09467.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier09467.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier09467.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.086 seconds
> compiling and loading fused kernels ...
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax_hip.cpp [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax_hip.h [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/compat.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/compat.h [skipped, no changes]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/type_shim.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/type_shim.h [skipped, no changes]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax_hip.hip [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax_hip.h [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/type_shim.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/type_shim.h [skipped, no changes]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/compat.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/compat.h [skipped, no changes]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax_hip.h [skipped, already hipified]
[92mSuccessfully preprocessed all matching files.[0m
Detected CUDA files, patching ldflags
Emitting ninja build file /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 99
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax.cpp -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax_hip.cpp [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax_cuda.cu -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax_hip.hip [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax_hip.h [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/type_shim.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/type_shim.h [skipped, no changes]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/compat.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/compat.h [skipped, no changes]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax_hip.h [skipped, already hipified]
[92mSuccessfully preprocessed all matching files.[0m
Detected CUDA files, patching ldflags
Emitting ninja build file /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 69
[1/1] c++ scaled_masked_softmax_hip.cuda.o scaled_masked_softmax_hip.o -shared -L/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/torch/lib -lc10 -lc10_hip -ltorch_cpu -ltorch_hip -ltorch -ltorch_python -L/opt/rocm-5.7.0/lib -lamdhip64 -o scaled_masked_softmax_cuda.so
Loading extension module scaled_masked_softmax_cuda...
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_softmax.cpp -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_softmax_hip.cpp [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_softmax_cuda.cu -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_softmax_hip.hip [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_upper_triang_masked_softmax_hip.h [skipped, already hipified]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/type_shim.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/type_shim.h [skipped, no changes]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/compat.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/compat.h [skipped, no changes]
/lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax.h -> /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/scaled_masked_softmax_hip.h [skipped, already hipified]
[92mSuccessfully preprocessed all matching files.[0m
Detected CUDA files, patching ldflags
Emitting ninja build file /lustre/orion/gen150/scratch/pinaster/smore/Megatron-DeepSpeed-tutel/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 69
ninja: no work to do.
Loading extension module scaled_softmax_cuda...
[92mSuccessfully preprocessed all matching files.[0m
[92mSuccessfully preprocessed all matching files.[0m
[92mSuccessfully preprocessed all matching files.[0m
[92mSuccessfully preprocessed all matching files.[0m
[92mSuccessfully preprocessed all matching files.[0m
[92mSuccessfully preprocessed all matching files.[0m
>>> done with compiling and loading fused kernels. Compilation time: 6.234 seconds
time to initialize megatron (seconds): 8.462
[after megatron is initialized] datetime: 2025-04-20 21:31:13 
building GPT model ...
[2025-04-20 21:31:13,510] [INFO] [utils.py:781:see_memory_usage] Before Building Model
[2025-04-20 21:31:13,511] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-04-20 21:31:13,511] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.57 GB, percent = 5.9%
world_size, rank, master_addr, local_rank: 8 0 100.64.201.123 0
using_mpi= False
#####
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffd14235cb0>
[2025-04-20 21:31:13,511] [WARNING] [partition_parameters.py:957:__init__] sequence_data_parallel_group' is deprecated and will be removed. Use 'data_parallel_group' instead.
world_size, rank, master_addr, local_rank: 8 1 100.64.201.123 1
using_mpi= False
#####
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffd3860d430>
[2025-04-20 21:31:13,511] [WARNING] [partition_parameters.py:957:__init__] sequence_data_parallel_group' is deprecated and will be removed. Use 'data_parallel_group' instead.
world_size, rank, master_addr, local_rank: 8 2 100.64.201.123 2
using_mpi= False
#####
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffd1423a6f0>
[2025-04-20 21:31:13,511] [WARNING] [partition_parameters.py:957:__init__] sequence_data_parallel_group' is deprecated and will be removed. Use 'data_parallel_group' instead.
world_size, rank, master_addr, local_rank: 8 3 100.64.201.123 3
using_mpi= False
#####
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffd1423a870>
[2025-04-20 21:31:13,511] [WARNING] [partition_parameters.py:957:__init__] sequence_data_parallel_group' is deprecated and will be removed. Use 'data_parallel_group' instead.
world_size, rank, master_addr, local_rank: 8 4 100.64.201.123 4
using_mpi= False
#####
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffd15b41cb0>
[2025-04-20 21:31:13,511] [WARNING] [partition_parameters.py:957:__init__] sequence_data_parallel_group' is deprecated and will be removed. Use 'data_parallel_group' instead.
world_size, rank, master_addr, local_rank: 8 6 100.64.201.123 6
using_mpi= False
#####
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffd1423aa30>
[2025-04-20 21:31:13,511] [WARNING] [partition_parameters.py:957:__init__] sequence_data_parallel_group' is deprecated and will be removed. Use 'data_parallel_group' instead.
world_size, rank, master_addr, local_rank: 8 5 100.64.201.123 5
using_mpi= False
#####
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffd1423ac70>
[2025-04-20 21:31:13,511] [WARNING] [partition_parameters.py:957:__init__] sequence_data_parallel_group' is deprecated and will be removed. Use 'data_parallel_group' instead.
world_size, rank, master_addr, local_rank: 8 7 100.64.201.123 7
using_mpi= False
#####
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffd1423a670>
[2025-04-20 21:31:13,511] [WARNING] [partition_parameters.py:957:__init__] sequence_data_parallel_group' is deprecated and will be removed. Use 'data_parallel_group' instead.
[2025-04-20 21:31:13,785] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,793] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,797] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,802] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,807] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,812] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,817] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,821] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,826] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,831] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,836] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,841] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,846] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,851] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,856] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,861] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,866] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,870] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,875] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,880] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,885] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,890] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,895] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,900] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,905] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,910] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,915] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,920] [INFO] [logging.py:128:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 8 | expert_parallel_size: 8 | num_shared_experts 0
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:13,992] [INFO] [utils.py:781:see_memory_usage] After Building Model
[2025-04-20 21:31:13,992] [INFO] [utils.py:782:see_memory_usage] MA 3.5 GB         Max_MA 3.51 GB         CA 3.98 GB         Max_CA 4 GB 
[2025-04-20 21:31:13,993] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.65 GB, percent = 5.9%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1869537280
/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:14,001] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:14,001] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:14,002] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:14,002] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
> learning rate decay style: cosine
DeepSpeed is enabled.
[2025-04-20 21:31:14,003] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.5+unknown, git-hash=unknown, git-branch=unknown
[2025-04-20 21:31:14,003] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:14,019] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:14,021] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
/lustre/orion/gen150/scratch/pinaster/miniconda3/envs/flash2/lib/python3.11/site-packages/apex/optimizers/fused_adam.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
Use sequential GEMM.
uneven all-to-all for num_local_experts > 1 is not implemented
[2025-04-20 21:31:14,022] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
No existing process group found, creating a new group named: ep_size_8
[2025-04-20 21:31:14,025] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert and data parallel groups with size 8
[2025-04-20 21:31:14,025] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [0]
[2025-04-20 21:31:14,026] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [1]
[2025-04-20 21:31:14,026] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [2]
[2025-04-20 21:31:14,026] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [3]
[2025-04-20 21:31:14,026] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [4]
[2025-04-20 21:31:14,026] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [5]
[2025-04-20 21:31:14,026] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [6]
[2025-04-20 21:31:14,026] [INFO] [logging.py:128:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [7]
[2025-04-20 21:31:14,026] [INFO] [logging.py:128:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [0, 1, 2, 3, 4, 5, 6, 7]
[2025-04-20 21:31:17,135] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-04-20 21:31:17,138] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-04-20 21:31:17,138] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-20 21:31:17,209] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-04-20 21:31:17,209] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2025-04-20 21:31:17,209] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2025-04-20 21:31:17,209] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-20 21:31:17,209] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-20 21:31:17,209] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-20 21:31:17,209] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-20 21:31:24,755] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-20 21:31:24,755] [INFO] [utils.py:782:see_memory_usage] MA 8.57 GB         Max_MA 8.58 GB         CA 8.59 GB         Max_CA 9 GB 
[2025-04-20 21:31:24,756] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 34.04 GB, percent = 6.8%
[2025-04-20 21:31:24,816] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-20 21:31:24,817] [INFO] [utils.py:782:see_memory_usage] MA 8.57 GB         Max_MA 13.66 GB         CA 13.68 GB         Max_CA 14 GB 
[2025-04-20 21:31:24,817] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 34.04 GB, percent = 6.8%
[2025-04-20 21:31:24,817] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized
[2025-04-20 21:31:24,873] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-20 21:31:24,874] [INFO] [utils.py:782:see_memory_usage] MA 8.57 GB         Max_MA 8.57 GB         CA 13.68 GB         Max_CA 14 GB 
[2025-04-20 21:31:24,874] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 34.04 GB, percent = 6.8%
[2025-04-20 21:31:24,886] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-20 21:31:24,886] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-04-20 21:31:24,886] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7ffd15b5da10>
[2025-04-20 21:31:24,886] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:31:24,889] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ffd509d3410>
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 220277, 'difficulty_step': 8}}
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   fp16_enabled ................. True
[2025-04-20 21:31:24,889] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 8
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 2048
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   loss_scale ................... 0
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   steps_per_print .............. 1
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   train_batch_size ............. 256
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  4
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   world_size ................... 8
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-20 21:31:24,890] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 1
[2025-04-20 21:31:24,890] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 1
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 2.048000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.202770e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false, 
    "profiling": {
        "enabled": true, 
        "profile_steps": [2], 
        "module_depth": -1, 
        "detailed": true, 
        "time_breakdown": true
    }
}
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-04-20 21:31:24 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      25600
    validation: 0
    test:       0
> building train, validation, and test datasets for GPT ...
Single data path provided for train, valid & test
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.012565 seconds
    number of documents: 27933
 > dataset split:
    train:
     document indices in [0, 27374) total of 27374 documents
    validation:
     document indices in [27374, 27933) total of 559 documents
    test:
     document indices in [27933, 27933) total of 0 documents
 > loading doc-idx mapping from /lustre/orion/world-shared/stf218/sajal/mtds/gptdata/index-cache/5943a9ec513ebaf2ce6aa145181457c0_doc_idx.npy
 > loading sample-idx mapping from /lustre/orion/world-shared/stf218/sajal/mtds/gptdata/index-cache/5943a9ec513ebaf2ce6aa145181457c0_sample_idx.npy
 > loading shuffle-idx mapping from /lustre/orion/world-shared/stf218/sajal/mtds/gptdata/index-cache/5943a9ec513ebaf2ce6aa145181457c0_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 28287
    total number of epochs: 6
 > loading doc-idx mapping from /lustre/orion/world-shared/stf218/sajal/mtds/gptdata/index-cache/aa8a3b2b80af2f4d58d677bd6c6a19a4_doc_idx.npy
 > loading sample-idx mapping from /lustre/orion/world-shared/stf218/sajal/mtds/gptdata/index-cache/aa8a3b2b80af2f4d58d677bd6c6a19a4_sample_idx.npy
 > loading shuffle-idx mapping from /lustre/orion/world-shared/stf218/sajal/mtds/gptdata/index-cache/aa8a3b2b80af2f4d58d677bd6c6a19a4_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 83
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2025-04-20 21:31:25 
done with setup ...
training ...
No existing process group found, creating a new group named: ep_size_8
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (10148.36, 11452.60)
    train/valid/test-data-iterators-setup ..........: (162.66, 165.98)
[before the start of training step] datetime: 2025-04-20 21:31:25 
[2025-04-20 21:32:06,183] [INFO] [logging.py:128:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
Max Memory Allocated across all ranks: 41168.26 MB
Max Memory Reserved across all ranks: 45518.00 MB
 iteration        1/     100 | consumed samples:          256 | consumed tokens:       524288 | elapsed time per iteration (ms): 40360.7 | learning rate: 0.000E+00 | global batch size:   256 | lm loss: 1.095979E+01 | moe loss: 3.541273E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 6.343 | TFLOPs: 17.31 |
[2025-04-20 21:32:20,818] [INFO] [logging.py:128:log_dist] [Rank 0] step=2, skipped=0, lr=[1.6777216e-07, 1.6777216e-07, 1.6777216e-07, 1.6777216e-07, 1.6777216e-07, 1.6777216e-07, 1.6777216e-07, 1.6777216e-07, 1.6777216e-07, 1.6777216e-07, 1.6777216e-07], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
Max Memory Allocated across all ranks: 51563.23 MB
Max Memory Reserved across all ranks: 57454.00 MB
 iteration        2/     100 | consumed samples:          512 | consumed tokens:      1048576 | elapsed time per iteration (ms): 14632.1 | learning rate: 1.678E-07 | global batch size:   256 | lm loss: 1.096001E+01 | moe loss: 3.546345E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.496 | TFLOPs: 47.74 |
[Rank 0] (after 2 iterations) memory (MB) | allocated: 19192.5390625 | max allocated: 51384.2568359375 | reserved: 55476.0 | max reserved: 55476.0
[2025-04-20 21:32:35,467] [INFO] [logging.py:128:log_dist] [Rank 0] step=3, skipped=0, lr=[3.3554432e-07, 3.3554432e-07, 3.3554432e-07, 3.3554432e-07, 3.3554432e-07, 3.3554432e-07, 3.3554432e-07, 3.3554432e-07, 3.3554432e-07, 3.3554432e-07, 3.3554432e-07], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:32:35,520] [INFO] [timer.py:264:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=125.03214750913105, CurrSamplesPerSec=125.03208644260653, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57454.00 MB
 iteration        3/     100 | consumed samples:          768 | consumed tokens:      1572864 | elapsed time per iteration (ms): 14673.8 | learning rate: 3.355E-07 | global batch size:   256 | lm loss: 1.096177E+01 | moe loss: 3.548623E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.446 | TFLOPs: 47.61 |
[2025-04-20 21:32:50,244] [INFO] [logging.py:128:log_dist] [Rank 0] step=4, skipped=0, lr=[5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07, 5.033164800000001e-07], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:32:50,296] [INFO] [timer.py:264:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=125.00235151732181, CurrSamplesPerSec=124.97250871495291, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57454.00 MB
 iteration        4/     100 | consumed samples:         1024 | consumed tokens:      2097152 | elapsed time per iteration (ms): 14780.8 | learning rate: 5.033E-07 | global batch size:   256 | lm loss: 1.095353E+01 | moe loss: 3.547285E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.320 | TFLOPs: 47.26 |
[2025-04-20 21:33:05,078] [INFO] [logging.py:128:log_dist] [Rank 0] step=5, skipped=0, lr=[6.7108864e-07, 6.7108864e-07, 6.7108864e-07, 6.7108864e-07, 6.7108864e-07, 6.7108864e-07, 6.7108864e-07, 6.7108864e-07, 6.7108864e-07, 6.7108864e-07, 6.7108864e-07], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:33:05,130] [INFO] [timer.py:264:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=124.28246837413913, CurrSamplesPerSec=122.86723453138677, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57454.00 MB
 iteration        5/     100 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 14833.2 | learning rate: 6.711E-07 | global batch size:   256 | lm loss: 1.087196E+01 | moe loss: 3.534728E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.259 | TFLOPs: 47.09 |
[2025-04-20 21:33:19,786] [INFO] [logging.py:128:log_dist] [Rank 0] step=6, skipped=0, lr=[8.388608e-07, 8.388608e-07, 8.388608e-07, 8.388608e-07, 8.388608e-07, 8.388608e-07, 8.388608e-07, 8.388608e-07, 8.388608e-07, 8.388608e-07, 8.388608e-07], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:33:19,838] [INFO] [timer.py:264:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=124.40984663784391, CurrSamplesPerSec=124.7934920411217, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57456.00 MB
 iteration        6/     100 | consumed samples:         1536 | consumed tokens:      3145728 | elapsed time per iteration (ms): 14711.2 | learning rate: 8.389E-07 | global batch size:   256 | lm loss: 1.074238E+01 | moe loss: 3.525305E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.402 | TFLOPs: 47.49 |
[2025-04-20 21:33:34,512] [INFO] [logging.py:128:log_dist] [Rank 0] step=7, skipped=0, lr=[1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06, 1.0066329600000002e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:33:34,565] [INFO] [timer.py:264:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=124.51346308374703, CurrSamplesPerSec=124.9295996376495, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57456.00 MB
 iteration        7/     100 | consumed samples:         1792 | consumed tokens:      3670016 | elapsed time per iteration (ms): 14726.4 | learning rate: 1.007E-06 | global batch size:   256 | lm loss: 1.063175E+01 | moe loss: 3.519599E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.384 | TFLOPs: 47.44 |
[2025-04-20 21:33:49,232] [INFO] [logging.py:128:log_dist] [Rank 0] step=8, skipped=0, lr=[1.17440512e-06, 1.17440512e-06, 1.17440512e-06, 1.17440512e-06, 1.17440512e-06, 1.17440512e-06, 1.17440512e-06, 1.17440512e-06, 1.17440512e-06, 1.17440512e-06, 1.17440512e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:33:49,285] [INFO] [timer.py:264:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=124.53516316975517, CurrSamplesPerSec=124.64371646709905, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57458.00 MB
 iteration        8/     100 | consumed samples:         2048 | consumed tokens:      4194304 | elapsed time per iteration (ms): 14720.5 | learning rate: 1.174E-06 | global batch size:   256 | lm loss: 1.048806E+01 | moe loss: 3.525865E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.391 | TFLOPs: 47.46 |
[2025-04-20 21:34:04,054] [INFO] [logging.py:128:log_dist] [Rank 0] step=9, skipped=0, lr=[1.34217728e-06, 1.34217728e-06, 1.34217728e-06, 1.34217728e-06, 1.34217728e-06, 1.34217728e-06, 1.34217728e-06, 1.34217728e-06, 1.34217728e-06, 1.34217728e-06, 1.34217728e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:34:04,106] [INFO] [timer.py:264:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=124.63200869178183, CurrSamplesPerSec=125.21619853079218, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57458.00 MB
 iteration        9/     100 | consumed samples:         2304 | consumed tokens:      4718592 | elapsed time per iteration (ms): 14822.2 | learning rate: 1.342E-06 | global batch size:   256 | lm loss: 1.029826E+01 | moe loss: 3.552887E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.271 | TFLOPs: 47.13 |
[2025-04-20 21:34:18,744] [INFO] [logging.py:128:log_dist] [Rank 0] step=10, skipped=0, lr=[1.50994944e-06, 1.50994944e-06, 1.50994944e-06, 1.50994944e-06, 1.50994944e-06, 1.50994944e-06, 1.50994944e-06, 1.50994944e-06, 1.50994944e-06, 1.50994944e-06, 1.50994944e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:34:18,796] [INFO] [timer.py:264:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=124.19834120068583, CurrSamplesPerSec=121.2451050413045, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57458.00 MB
 iteration       10/     100 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 14690.9 | learning rate: 1.510E-06 | global batch size:   256 | lm loss: 1.013267E+01 | moe loss: 3.590926E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.426 | TFLOPs: 47.55 |
[2025-04-20 21:34:33,259] [INFO] [logging.py:128:log_dist] [Rank 0] step=11, skipped=0, lr=[1.6777216e-06, 1.6777216e-06, 1.6777216e-06, 1.6777216e-06, 1.6777216e-06, 1.6777216e-06, 1.6777216e-06, 1.6777216e-06, 1.6777216e-06, 1.6777216e-06, 1.6777216e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:34:33,311] [INFO] [timer.py:264:stop] epoch=0/micro_step=11/global_step=11, RunningAvgSamplesPerSec=124.4293401487117, CurrSamplesPerSec=126.308670688309, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57464.00 MB
 iteration       11/     100 | consumed samples:         2816 | consumed tokens:      5767168 | elapsed time per iteration (ms): 14514.0 | learning rate: 1.678E-06 | global batch size:   256 | lm loss: 9.939994E+00 | moe loss: 3.662636E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.638 | TFLOPs: 48.13 |
[2025-04-20 21:34:47,748] [INFO] [logging.py:128:log_dist] [Rank 0] step=12, skipped=0, lr=[1.84549376e-06, 1.84549376e-06, 1.84549376e-06, 1.84549376e-06, 1.84549376e-06, 1.84549376e-06, 1.84549376e-06, 1.84549376e-06, 1.84549376e-06, 1.84549376e-06, 1.84549376e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:34:47,800] [INFO] [timer.py:264:stop] epoch=0/micro_step=12/global_step=12, RunningAvgSamplesPerSec=124.40402187622715, CurrSamplesPerSec=124.17655999008171, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57466.00 MB
 iteration       12/     100 | consumed samples:         3072 | consumed tokens:      6291456 | elapsed time per iteration (ms): 14491.0 | learning rate: 1.845E-06 | global batch size:   256 | lm loss: 9.763584E+00 | moe loss: 3.753300E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.666 | TFLOPs: 48.21 |
[2025-04-20 21:35:02,085] [INFO] [logging.py:128:log_dist] [Rank 0] step=13, skipped=0, lr=[2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06, 2.0132659200000003e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:35:02,137] [INFO] [timer.py:264:stop] epoch=0/micro_step=13/global_step=13, RunningAvgSamplesPerSec=124.70336568038421, CurrSamplesPerSec=127.77792511010858, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       13/     100 | consumed samples:         3328 | consumed tokens:      6815744 | elapsed time per iteration (ms): 14336.7 | learning rate: 2.013E-06 | global batch size:   256 | lm loss: 9.590774E+00 | moe loss: 3.845109E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.856 | TFLOPs: 48.73 |
[2025-04-20 21:35:16,356] [INFO] [logging.py:128:log_dist] [Rank 0] step=14, skipped=0, lr=[2.18103808e-06, 2.18103808e-06, 2.18103808e-06, 2.18103808e-06, 2.18103808e-06, 2.18103808e-06, 2.18103808e-06, 2.18103808e-06, 2.18103808e-06, 2.18103808e-06, 2.18103808e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:35:16,409] [INFO] [timer.py:264:stop] epoch=0/micro_step=14/global_step=14, RunningAvgSamplesPerSec=125.01017880695665, CurrSamplesPerSec=128.48747269395923, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       14/     100 | consumed samples:         3584 | consumed tokens:      7340032 | elapsed time per iteration (ms): 14272.3 | learning rate: 2.181E-06 | global batch size:   256 | lm loss: 9.464109E+00 | moe loss: 3.931862E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.937 | TFLOPs: 48.95 |
[2025-04-20 21:35:30,620] [INFO] [logging.py:128:log_dist] [Rank 0] step=15, skipped=0, lr=[2.34881024e-06, 2.34881024e-06, 2.34881024e-06, 2.34881024e-06, 2.34881024e-06, 2.34881024e-06, 2.34881024e-06, 2.34881024e-06, 2.34881024e-06, 2.34881024e-06, 2.34881024e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:35:30,672] [INFO] [timer.py:264:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=125.31112417729204, CurrSamplesPerSec=129.03878547073444, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       15/     100 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 14262.9 | learning rate: 2.349E-06 | global batch size:   256 | lm loss: 9.323806E+00 | moe loss: 3.976759E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.949 | TFLOPs: 48.98 |
[2025-04-20 21:35:44,882] [INFO] [logging.py:128:log_dist] [Rank 0] step=16, skipped=0, lr=[2.5165824e-06, 2.5165824e-06, 2.5165824e-06, 2.5165824e-06, 2.5165824e-06, 2.5165824e-06, 2.5165824e-06, 2.5165824e-06, 2.5165824e-06, 2.5165824e-06, 2.5165824e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:35:44,935] [INFO] [timer.py:264:stop] epoch=0/micro_step=16/global_step=16, RunningAvgSamplesPerSec=125.56590107845912, CurrSamplesPerSec=128.97477143390122, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       16/     100 | consumed samples:         4096 | consumed tokens:      8388608 | elapsed time per iteration (ms): 14263.5 | learning rate: 2.517E-06 | global batch size:   256 | lm loss: 9.241631E+00 | moe loss: 3.990301E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.948 | TFLOPs: 48.98 |
[2025-04-20 21:35:59,082] [INFO] [logging.py:128:log_dist] [Rank 0] step=17, skipped=0, lr=[2.68435456e-06, 2.68435456e-06, 2.68435456e-06, 2.68435456e-06, 2.68435456e-06, 2.68435456e-06, 2.68435456e-06, 2.68435456e-06, 2.68435456e-06, 2.68435456e-06, 2.68435456e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:35:59,134] [INFO] [timer.py:264:stop] epoch=0/micro_step=17/global_step=17, RunningAvgSamplesPerSec=125.78187809539502, CurrSamplesPerSec=128.88542858612965, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       17/     100 | consumed samples:         4352 | consumed tokens:      8912896 | elapsed time per iteration (ms): 14198.6 | learning rate: 2.684E-06 | global batch size:   256 | lm loss: 9.173053E+00 | moe loss: 3.992205E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.030 | TFLOPs: 49.20 |
[2025-04-20 21:36:13,240] [INFO] [logging.py:128:log_dist] [Rank 0] step=18, skipped=0, lr=[2.85212672e-06, 2.85212672e-06, 2.85212672e-06, 2.85212672e-06, 2.85212672e-06, 2.85212672e-06, 2.85212672e-06, 2.85212672e-06, 2.85212672e-06, 2.85212672e-06, 2.85212672e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:36:13,293] [INFO] [timer.py:264:stop] epoch=0/micro_step=18/global_step=18, RunningAvgSamplesPerSec=125.98632023078582, CurrSamplesPerSec=129.1346306797873, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       18/     100 | consumed samples:         4608 | consumed tokens:      9437184 | elapsed time per iteration (ms): 14160.0 | learning rate: 2.852E-06 | global batch size:   256 | lm loss: 9.076735E+00 | moe loss: 3.975400E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.079 | TFLOPs: 49.33 |
[2025-04-20 21:36:27,360] [INFO] [logging.py:128:log_dist] [Rank 0] step=19, skipped=0, lr=[3.01989888e-06, 3.01989888e-06, 3.01989888e-06, 3.01989888e-06, 3.01989888e-06, 3.01989888e-06, 3.01989888e-06, 3.01989888e-06, 3.01989888e-06, 3.01989888e-06, 3.01989888e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:36:27,413] [INFO] [timer.py:264:stop] epoch=0/micro_step=19/global_step=19, RunningAvgSamplesPerSec=126.18719094185896, CurrSamplesPerSec=129.49044939160885, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       19/     100 | consumed samples:         4864 | consumed tokens:      9961472 | elapsed time per iteration (ms): 14118.5 | learning rate: 3.020E-06 | global batch size:   256 | lm loss: 8.987990E+00 | moe loss: 3.951198E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.132 | TFLOPs: 49.48 |
[2025-04-20 21:36:41,457] [INFO] [logging.py:128:log_dist] [Rank 0] step=20, skipped=0, lr=[3.18767104e-06, 3.18767104e-06, 3.18767104e-06, 3.18767104e-06, 3.18767104e-06, 3.18767104e-06, 3.18767104e-06, 3.18767104e-06, 3.18767104e-06, 3.18767104e-06, 3.18767104e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:36:41,509] [INFO] [timer.py:264:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=126.38524221884228, CurrSamplesPerSec=129.84977324877963, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       20/     100 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 14098.8 | learning rate: 3.188E-06 | global batch size:   256 | lm loss: 8.918373E+00 | moe loss: 3.927920E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.158 | TFLOPs: 49.55 |
[2025-04-20 21:36:55,658] [INFO] [logging.py:128:log_dist] [Rank 0] step=21, skipped=0, lr=[3.3554432e-06, 3.3554432e-06, 3.3554432e-06, 3.3554432e-06, 3.3554432e-06, 3.3554432e-06, 3.3554432e-06, 3.3554432e-06, 3.3554432e-06, 3.3554432e-06, 3.3554432e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:36:55,710] [INFO] [timer.py:264:stop] epoch=0/micro_step=21/global_step=21, RunningAvgSamplesPerSec=126.54725354934371, CurrSamplesPerSec=129.5360960388396, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       21/     100 | consumed samples:         5376 | consumed tokens:     11010048 | elapsed time per iteration (ms): 14199.3 | learning rate: 3.355E-06 | global batch size:   256 | lm loss: 8.887044E+00 | moe loss: 3.878409E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.029 | TFLOPs: 49.20 |
[2025-04-20 21:37:09,894] [INFO] [logging.py:128:log_dist] [Rank 0] step=22, skipped=0, lr=[3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06, 3.5232153600000002e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:37:09,947] [INFO] [timer.py:264:stop] epoch=0/micro_step=22/global_step=22, RunningAvgSamplesPerSec=126.67416209830542, CurrSamplesPerSec=129.13466174080492, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       22/     100 | consumed samples:         5632 | consumed tokens:     11534336 | elapsed time per iteration (ms): 14238.0 | learning rate: 3.523E-06 | global batch size:   256 | lm loss: 8.827065E+00 | moe loss: 3.835122E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.980 | TFLOPs: 49.06 |
[2025-04-20 21:37:24,092] [INFO] [logging.py:128:log_dist] [Rank 0] step=23, skipped=0, lr=[3.69098752e-06, 3.69098752e-06, 3.69098752e-06, 3.69098752e-06, 3.69098752e-06, 3.69098752e-06, 3.69098752e-06, 3.69098752e-06, 3.69098752e-06, 3.69098752e-06, 3.69098752e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:37:24,145] [INFO] [timer.py:264:stop] epoch=0/micro_step=23/global_step=23, RunningAvgSamplesPerSec=126.78445145424554, CurrSamplesPerSec=129.03121827414674, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       23/     100 | consumed samples:         5888 | consumed tokens:     12058624 | elapsed time per iteration (ms): 14196.7 | learning rate: 3.691E-06 | global batch size:   256 | lm loss: 8.738462E+00 | moe loss: 3.787594E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.032 | TFLOPs: 49.21 |
[2025-04-20 21:37:38,402] [INFO] [logging.py:128:log_dist] [Rank 0] step=24, skipped=0, lr=[3.85875968e-06, 3.85875968e-06, 3.85875968e-06, 3.85875968e-06, 3.85875968e-06, 3.85875968e-06, 3.85875968e-06, 3.85875968e-06, 3.85875968e-06, 3.85875968e-06, 3.85875968e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:37:38,454] [INFO] [timer.py:264:stop] epoch=0/micro_step=24/global_step=24, RunningAvgSamplesPerSec=126.84415544477764, CurrSamplesPerSec=128.11099407821678, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       24/     100 | consumed samples:         6144 | consumed tokens:     12582912 | elapsed time per iteration (ms): 14311.2 | learning rate: 3.859E-06 | global batch size:   256 | lm loss: 8.694723E+00 | moe loss: 3.726075E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.888 | TFLOPs: 48.81 |
[2025-04-20 21:37:52,836] [INFO] [logging.py:128:log_dist] [Rank 0] step=25, skipped=0, lr=[4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06, 4.026531840000001e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:37:52,888] [INFO] [timer.py:264:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=126.85604228437839, CurrSamplesPerSec=127.11805445328619, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       25/     100 | consumed samples:         6400 | consumed tokens:     13107200 | elapsed time per iteration (ms): 14432.5 | learning rate: 4.027E-06 | global batch size:   256 | lm loss: 8.657719E+00 | moe loss: 3.661080E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.738 | TFLOPs: 48.40 |
[2025-04-20 21:38:07,511] [INFO] [logging.py:128:log_dist] [Rank 0] step=26, skipped=0, lr=[4.194304e-06, 4.194304e-06, 4.194304e-06, 4.194304e-06, 4.194304e-06, 4.194304e-06, 4.194304e-06, 4.194304e-06, 4.194304e-06, 4.194304e-06, 4.194304e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:38:07,563] [INFO] [timer.py:264:stop] epoch=0/micro_step=26/global_step=26, RunningAvgSamplesPerSec=126.81226115704318, CurrSamplesPerSec=125.81350839684092, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       26/     100 | consumed samples:         6656 | consumed tokens:     13631488 | elapsed time per iteration (ms): 14675.6 | learning rate: 4.194E-06 | global batch size:   256 | lm loss: 8.595143E+00 | moe loss: 3.595620E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.444 | TFLOPs: 47.60 |
[2025-04-20 21:38:22,347] [INFO] [logging.py:128:log_dist] [Rank 0] step=27, skipped=0, lr=[4.36207616e-06, 4.36207616e-06, 4.36207616e-06, 4.36207616e-06, 4.36207616e-06, 4.36207616e-06, 4.36207616e-06, 4.36207616e-06, 4.36207616e-06, 4.36207616e-06, 4.36207616e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:38:22,399] [INFO] [timer.py:264:stop] epoch=0/micro_step=27/global_step=27, RunningAvgSamplesPerSec=126.7244546067222, CurrSamplesPerSec=124.65291948125405, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       27/     100 | consumed samples:         6912 | consumed tokens:     14155776 | elapsed time per iteration (ms): 14835.4 | learning rate: 4.362E-06 | global batch size:   256 | lm loss: 8.551815E+00 | moe loss: 3.528138E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.256 | TFLOPs: 47.09 |
[2025-04-20 21:38:37,268] [INFO] [logging.py:128:log_dist] [Rank 0] step=28, skipped=0, lr=[4.52984832e-06, 4.52984832e-06, 4.52984832e-06, 4.52984832e-06, 4.52984832e-06, 4.52984832e-06, 4.52984832e-06, 4.52984832e-06, 4.52984832e-06, 4.52984832e-06, 4.52984832e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:38:37,320] [INFO] [timer.py:264:stop] epoch=0/micro_step=28/global_step=28, RunningAvgSamplesPerSec=126.59311247690177, CurrSamplesPerSec=123.39574821532617, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       28/     100 | consumed samples:         7168 | consumed tokens:     14680064 | elapsed time per iteration (ms): 14920.9 | learning rate: 4.530E-06 | global batch size:   256 | lm loss: 8.509456E+00 | moe loss: 3.461273E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.157 | TFLOPs: 46.82 |
[2025-04-20 21:38:52,293] [INFO] [logging.py:128:log_dist] [Rank 0] step=29, skipped=0, lr=[4.69762048e-06, 4.69762048e-06, 4.69762048e-06, 4.69762048e-06, 4.69762048e-06, 4.69762048e-06, 4.69762048e-06, 4.69762048e-06, 4.69762048e-06, 4.69762048e-06, 4.69762048e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:38:52,345] [INFO] [timer.py:264:stop] epoch=0/micro_step=29/global_step=29, RunningAvgSamplesPerSec=126.41349987201033, CurrSamplesPerSec=121.9160455357182, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51621.20 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       29/     100 | consumed samples:         7424 | consumed tokens:     15204352 | elapsed time per iteration (ms): 15026.5 | learning rate: 4.698E-06 | global batch size:   256 | lm loss: 8.443656E+00 | moe loss: 3.404591E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.037 | TFLOPs: 46.49 |
[2025-04-20 21:39:07,433] [INFO] [logging.py:128:log_dist] [Rank 0] step=30, skipped=0, lr=[4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06, 4.8653926400000004e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:39:07,486] [INFO] [timer.py:264:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=126.24822323956523, CurrSamplesPerSec=121.94348800968564, MemAllocated=18.74GB, MaxMemAllocated=50.18GB
Max Memory Allocated across all ranks: 51734.68 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       30/     100 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 15140.6 | learning rate: 4.865E-06 | global batch size:   256 | lm loss: 8.401659E+00 | moe loss: 3.352304E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.908 | TFLOPs: 46.14 |
[2025-04-20 21:39:22,655] [INFO] [logging.py:128:log_dist] [Rank 0] step=31, skipped=0, lr=[5.0331648e-06, 5.0331648e-06, 5.0331648e-06, 5.0331648e-06, 5.0331648e-06, 5.0331648e-06, 5.0331648e-06, 5.0331648e-06, 5.0331648e-06, 5.0331648e-06, 5.0331648e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:39:22,708] [INFO] [timer.py:264:stop] epoch=0/micro_step=31/global_step=31, RunningAvgSamplesPerSec=126.06359965380004, CurrSamplesPerSec=121.1046907962297, MemAllocated=18.74GB, MaxMemAllocated=50.44GB
Max Memory Allocated across all ranks: 52105.49 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       31/     100 | consumed samples:         7936 | consumed tokens:     16252928 | elapsed time per iteration (ms): 15221.1 | learning rate: 5.033E-06 | global batch size:   256 | lm loss: 8.361297E+00 | moe loss: 3.303502E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.819 | TFLOPs: 45.89 |
[2025-04-20 21:39:38,013] [INFO] [logging.py:128:log_dist] [Rank 0] step=32, skipped=0, lr=[5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06, 5.200936960000001e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:39:38,066] [INFO] [timer.py:264:stop] epoch=0/micro_step=32/global_step=32, RunningAvgSamplesPerSec=125.87786817129103, CurrSamplesPerSec=120.71991108916424, MemAllocated=18.74GB, MaxMemAllocated=50.73GB
Max Memory Allocated across all ranks: 52463.66 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       32/     100 | consumed samples:         8192 | consumed tokens:     16777216 | elapsed time per iteration (ms): 15357.0 | learning rate: 5.201E-06 | global batch size:   256 | lm loss: 8.300890E+00 | moe loss: 3.259461E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.670 | TFLOPs: 45.49 |
[2025-04-20 21:39:53,526] [INFO] [logging.py:128:log_dist] [Rank 0] step=33, skipped=0, lr=[5.36870912e-06, 5.36870912e-06, 5.36870912e-06, 5.36870912e-06, 5.36870912e-06, 5.36870912e-06, 5.36870912e-06, 5.36870912e-06, 5.36870912e-06, 5.36870912e-06, 5.36870912e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:39:53,578] [INFO] [timer.py:264:stop] epoch=0/micro_step=33/global_step=33, RunningAvgSamplesPerSec=125.53470647106134, CurrSamplesPerSec=116.0440517339601, MemAllocated=18.74GB, MaxMemAllocated=50.94GB
Max Memory Allocated across all ranks: 52724.71 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       33/     100 | consumed samples:         8448 | consumed tokens:     17301504 | elapsed time per iteration (ms): 15511.9 | learning rate: 5.369E-06 | global batch size:   256 | lm loss: 8.267351E+00 | moe loss: 3.225197E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.503 | TFLOPs: 45.03 |
[2025-04-20 21:40:08,999] [INFO] [logging.py:128:log_dist] [Rank 0] step=34, skipped=0, lr=[5.53648128e-06, 5.53648128e-06, 5.53648128e-06, 5.53648128e-06, 5.53648128e-06, 5.53648128e-06, 5.53648128e-06, 5.53648128e-06, 5.53648128e-06, 5.53648128e-06, 5.53648128e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:40:09,051] [INFO] [timer.py:264:stop] epoch=0/micro_step=34/global_step=34, RunningAvgSamplesPerSec=125.34038918034845, CurrSamplesPerSec=119.60121567680719, MemAllocated=18.74GB, MaxMemAllocated=51.1GB
Max Memory Allocated across all ranks: 53011.39 MB
Max Memory Reserved across all ranks: 57476.00 MB
 iteration       34/     100 | consumed samples:         8704 | consumed tokens:     17825792 | elapsed time per iteration (ms): 15474.5 | learning rate: 5.536E-06 | global batch size:   256 | lm loss: 8.219980E+00 | moe loss: 3.191894E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.543 | TFLOPs: 45.14 |
[2025-04-20 21:40:24,520] [INFO] [logging.py:128:log_dist] [Rank 0] step=35, skipped=0, lr=[5.70425344e-06, 5.70425344e-06, 5.70425344e-06, 5.70425344e-06, 5.70425344e-06, 5.70425344e-06, 5.70425344e-06, 5.70425344e-06, 5.70425344e-06, 5.70425344e-06, 5.70425344e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:40:24,573] [INFO] [timer.py:264:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=125.14029113340732, CurrSamplesPerSec=119.05803566285782, MemAllocated=18.74GB, MaxMemAllocated=51.23GB
Max Memory Allocated across all ranks: 53238.02 MB
Max Memory Reserved across all ranks: 57716.00 MB
 iteration       35/     100 | consumed samples:         8960 | consumed tokens:     18350080 | elapsed time per iteration (ms): 15521.1 | learning rate: 5.704E-06 | global batch size:   256 | lm loss: 8.168963E+00 | moe loss: 3.162953E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.494 | TFLOPs: 45.01 |
[2025-04-20 21:40:40,092] [INFO] [logging.py:128:log_dist] [Rank 0] step=36, skipped=0, lr=[5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06, 5.8720255999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:40:40,145] [INFO] [timer.py:264:stop] epoch=0/micro_step=36/global_step=36, RunningAvgSamplesPerSec=124.93608222390307, CurrSamplesPerSec=118.55191890124391, MemAllocated=18.74GB, MaxMemAllocated=51.41GB
Max Memory Allocated across all ranks: 53459.09 MB
Max Memory Reserved across all ranks: 57716.00 MB
 iteration       36/     100 | consumed samples:         9216 | consumed tokens:     18874368 | elapsed time per iteration (ms): 15572.1 | learning rate: 5.872E-06 | global batch size:   256 | lm loss: 8.115337E+00 | moe loss: 3.135245E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.440 | TFLOPs: 44.86 |
[2025-04-20 21:40:55,796] [INFO] [logging.py:128:log_dist] [Rank 0] step=37, skipped=0, lr=[6.03979776e-06, 6.03979776e-06, 6.03979776e-06, 6.03979776e-06, 6.03979776e-06, 6.03979776e-06, 6.03979776e-06, 6.03979776e-06, 6.03979776e-06, 6.03979776e-06, 6.03979776e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:40:55,848] [INFO] [timer.py:264:stop] epoch=0/micro_step=37/global_step=37, RunningAvgSamplesPerSec=124.7331195909018, CurrSamplesPerSec=118.20416379423766, MemAllocated=18.74GB, MaxMemAllocated=51.64GB
Max Memory Allocated across all ranks: 53665.35 MB
Max Memory Reserved across all ranks: 57716.00 MB
 iteration       37/     100 | consumed samples:         9472 | consumed tokens:     19398656 | elapsed time per iteration (ms): 15703.8 | learning rate: 6.040E-06 | global batch size:   256 | lm loss: 8.052433E+00 | moe loss: 3.114799E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.302 | TFLOPs: 44.48 |
[2025-04-20 21:41:11,461] [INFO] [logging.py:128:log_dist] [Rank 0] step=38, skipped=0, lr=[6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06, 6.207569920000001e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:41:11,514] [INFO] [timer.py:264:stop] epoch=0/micro_step=38/global_step=38, RunningAvgSamplesPerSec=124.53793645804762, CurrSamplesPerSec=118.0713239877974, MemAllocated=18.74GB, MaxMemAllocated=51.9GB
Max Memory Allocated across all ranks: 53855.62 MB
Max Memory Reserved across all ranks: 58164.00 MB
 iteration       38/     100 | consumed samples:         9728 | consumed tokens:     19922944 | elapsed time per iteration (ms): 15665.2 | learning rate: 6.208E-06 | global batch size:   256 | lm loss: 8.036002E+00 | moe loss: 3.081978E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.342 | TFLOPs: 44.59 |
[2025-04-20 21:41:27,204] [INFO] [logging.py:128:log_dist] [Rank 0] step=39, skipped=0, lr=[6.37534208e-06, 6.37534208e-06, 6.37534208e-06, 6.37534208e-06, 6.37534208e-06, 6.37534208e-06, 6.37534208e-06, 6.37534208e-06, 6.37534208e-06, 6.37534208e-06, 6.37534208e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:41:27,256] [INFO] [timer.py:264:stop] epoch=0/micro_step=39/global_step=39, RunningAvgSamplesPerSec=124.34972494208881, CurrSamplesPerSec=117.93338073239659, MemAllocated=18.74GB, MaxMemAllocated=52.13GB
Max Memory Allocated across all ranks: 54000.01 MB
Max Memory Reserved across all ranks: 58164.00 MB
 iteration       39/     100 | consumed samples:         9984 | consumed tokens:     20447232 | elapsed time per iteration (ms): 15740.9 | learning rate: 6.375E-06 | global batch size:   256 | lm loss: 7.988016E+00 | moe loss: 3.060054E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.263 | TFLOPs: 44.38 |
[2025-04-20 21:41:42,889] [INFO] [logging.py:128:log_dist] [Rank 0] step=40, skipped=0, lr=[6.54311424e-06, 6.54311424e-06, 6.54311424e-06, 6.54311424e-06, 6.54311424e-06, 6.54311424e-06, 6.54311424e-06, 6.54311424e-06, 6.54311424e-06, 6.54311424e-06, 6.54311424e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:41:42,938] [INFO] [timer.py:264:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=124.16883335842647, CurrSamplesPerSec=117.82687301779309, MemAllocated=18.74GB, MaxMemAllocated=52.33GB
Max Memory Allocated across all ranks: 54067.27 MB
Max Memory Reserved across all ranks: 58164.00 MB
 iteration       40/     100 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 15683.7 | learning rate: 6.543E-06 | global batch size:   256 | lm loss: 7.951405E+00 | moe loss: 3.041939E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.323 | TFLOPs: 44.54 |
[2025-04-20 21:41:58,582] [INFO] [logging.py:128:log_dist] [Rank 0] step=41, skipped=0, lr=[6.7108864e-06, 6.7108864e-06, 6.7108864e-06, 6.7108864e-06, 6.7108864e-06, 6.7108864e-06, 6.7108864e-06, 6.7108864e-06, 6.7108864e-06, 6.7108864e-06, 6.7108864e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:41:58,631] [INFO] [timer.py:264:stop] epoch=0/micro_step=41/global_step=41, RunningAvgSamplesPerSec=123.99253644004098, CurrSamplesPerSec=117.64516797514364, MemAllocated=18.74GB, MaxMemAllocated=52.59GB
Max Memory Allocated across all ranks: 54141.77 MB
Max Memory Reserved across all ranks: 58164.00 MB
 iteration       41/     100 | consumed samples:        10496 | consumed tokens:     21495808 | elapsed time per iteration (ms): 15690.4 | learning rate: 6.711E-06 | global batch size:   256 | lm loss: 7.903470E+00 | moe loss: 3.020125E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.316 | TFLOPs: 44.52 |
[2025-04-20 21:42:14,305] [INFO] [logging.py:128:log_dist] [Rank 0] step=42, skipped=0, lr=[6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06, 6.8786585599999995e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:42:14,357] [INFO] [timer.py:264:stop] epoch=0/micro_step=42/global_step=42, RunningAvgSamplesPerSec=123.80767675705474, CurrSamplesPerSec=117.00441577389289, MemAllocated=18.74GB, MaxMemAllocated=52.73GB
Max Memory Allocated across all ranks: 54253.11 MB
Max Memory Reserved across all ranks: 58654.00 MB
 iteration       42/     100 | consumed samples:        10752 | consumed tokens:     22020096 | elapsed time per iteration (ms): 15728.4 | learning rate: 6.879E-06 | global batch size:   256 | lm loss: 7.856673E+00 | moe loss: 3.006512E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.276 | TFLOPs: 44.41 |
[2025-04-20 21:42:30,097] [INFO] [logging.py:128:log_dist] [Rank 0] step=43, skipped=0, lr=[7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06, 7.0464307200000005e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:42:30,149] [INFO] [timer.py:264:stop] epoch=0/micro_step=43/global_step=43, RunningAvgSamplesPerSec=123.64968879812712, CurrSamplesPerSec=117.64469105212747, MemAllocated=18.74GB, MaxMemAllocated=52.93GB
Max Memory Allocated across all ranks: 54383.14 MB
Max Memory Reserved across all ranks: 58654.00 MB
 iteration       43/     100 | consumed samples:        11008 | consumed tokens:     22544384 | elapsed time per iteration (ms): 15791.5 | learning rate: 7.046E-06 | global batch size:   256 | lm loss: 7.824192E+00 | moe loss: 2.990915E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.211 | TFLOPs: 44.24 |
[2025-04-20 21:42:45,910] [INFO] [logging.py:128:log_dist] [Rank 0] step=44, skipped=0, lr=[7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06, 7.214202880000001e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:42:45,962] [INFO] [timer.py:264:stop] epoch=0/micro_step=44/global_step=44, RunningAvgSamplesPerSec=123.50039970234143, CurrSamplesPerSec=117.67523481911917, MemAllocated=18.74GB, MaxMemAllocated=53.1GB
Max Memory Allocated across all ranks: 54532.68 MB
Max Memory Reserved across all ranks: 59288.00 MB
 iteration       44/     100 | consumed samples:        11264 | consumed tokens:     23068672 | elapsed time per iteration (ms): 15813.6 | learning rate: 7.214E-06 | global batch size:   256 | lm loss: 7.812101E+00 | moe loss: 2.977861E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.189 | TFLOPs: 44.18 |
[2025-04-20 21:43:01,837] [INFO] [logging.py:128:log_dist] [Rank 0] step=45, skipped=0, lr=[7.38197504e-06, 7.38197504e-06, 7.38197504e-06, 7.38197504e-06, 7.38197504e-06, 7.38197504e-06, 7.38197504e-06, 7.38197504e-06, 7.38197504e-06, 7.38197504e-06, 7.38197504e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:43:01,889] [INFO] [timer.py:264:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=123.34407109891572, CurrSamplesPerSec=117.11755237047093, MemAllocated=18.74GB, MaxMemAllocated=53.26GB
Max Memory Allocated across all ranks: 54771.39 MB
Max Memory Reserved across all ranks: 59684.00 MB
 iteration       45/     100 | consumed samples:        11520 | consumed tokens:     23592960 | elapsed time per iteration (ms): 15925.0 | learning rate: 7.382E-06 | global batch size:   256 | lm loss: 7.744018E+00 | moe loss: 2.963615E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.075 | TFLOPs: 43.87 |
[2025-04-20 21:43:17,709] [INFO] [logging.py:128:log_dist] [Rank 0] step=46, skipped=0, lr=[7.5497472e-06, 7.5497472e-06, 7.5497472e-06, 7.5497472e-06, 7.5497472e-06, 7.5497472e-06, 7.5497472e-06, 7.5497472e-06, 7.5497472e-06, 7.5497472e-06, 7.5497472e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:43:17,762] [INFO] [timer.py:264:stop] epoch=0/micro_step=46/global_step=46, RunningAvgSamplesPerSec=123.18290214035343, CurrSamplesPerSec=116.62983015073387, MemAllocated=18.74GB, MaxMemAllocated=53.45GB
Max Memory Allocated across all ranks: 54914.82 MB
Max Memory Reserved across all ranks: 59684.00 MB
 iteration       46/     100 | consumed samples:        11776 | consumed tokens:     24117248 | elapsed time per iteration (ms): 15874.7 | learning rate: 7.550E-06 | global batch size:   256 | lm loss: 7.704115E+00 | moe loss: 2.948468E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.126 | TFLOPs: 44.01 |
[2025-04-20 21:43:33,598] [INFO] [logging.py:128:log_dist] [Rank 0] step=47, skipped=0, lr=[7.71751936e-06, 7.71751936e-06, 7.71751936e-06, 7.71751936e-06, 7.71751936e-06, 7.71751936e-06, 7.71751936e-06, 7.71751936e-06, 7.71751936e-06, 7.71751936e-06, 7.71751936e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:43:33,650] [INFO] [timer.py:264:stop] epoch=0/micro_step=47/global_step=47, RunningAvgSamplesPerSec=123.03073561676773, CurrSamplesPerSec=116.68834918408676, MemAllocated=18.74GB, MaxMemAllocated=53.58GB
Max Memory Allocated across all ranks: 55021.73 MB
Max Memory Reserved across all ranks: 59684.00 MB
 iteration       47/     100 | consumed samples:        12032 | consumed tokens:     24641536 | elapsed time per iteration (ms): 15886.3 | learning rate: 7.718E-06 | global batch size:   256 | lm loss: 7.688173E+00 | moe loss: 2.935303E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.114 | TFLOPs: 43.97 |
[2025-04-20 21:43:49,468] [INFO] [logging.py:128:log_dist] [Rank 0] step=48, skipped=0, lr=[7.88529152e-06, 7.88529152e-06, 7.88529152e-06, 7.88529152e-06, 7.88529152e-06, 7.88529152e-06, 7.88529152e-06, 7.88529152e-06, 7.88529152e-06, 7.88529152e-06, 7.88529152e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2025-04-20 21:43:49,521] [INFO] [timer.py:264:stop] epoch=0/micro_step=48/global_step=48, RunningAvgSamplesPerSec=122.88035502819967, CurrSamplesPerSec=116.4738259070649, MemAllocated=18.74GB, MaxMemAllocated=53.63GB
Max Memory Allocated across all ranks: 55140.05 MB
Max Memory Reserved across all ranks: 59684.00 MB
 iteration       48/     100 | consumed samples:        12288 | consumed tokens:     25165824 | elapsed time per iteration (ms): 15871.9 | learning rate: 7.885E-06 | global batch size:   256 | lm loss: 7.644805E+00 | moe loss: 2.927047E-01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.129 | TFLOPs: 44.01 |
